{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why run locally?\n",
    "\n",
    "1. privacy - we have no way to verify that data we send to the API's is not collected\n",
    "2. useful in offline situations\n",
    "\n",
    "## Local speech - to - text\n",
    "\n",
    "as of 2023, [Whisper](https://openai.com/research/whisper) remains the best open source (and free) speech to text model\n",
    "\n",
    "OpenAI has provided a simple package to help with running their model \n",
    "\n",
    "https://github.com/openai/whisper\n",
    "\n",
    "However it is rather slow because it runs on not so efficient for this kind of task PyTorch\n",
    "\n",
    "As an alternative I can recommend this package:\n",
    "\n",
    "https://github.com/guillaumekln/faster-whisper\n",
    "\n",
    "It uses different backend - [CTranslate2](https://github.com/OpenNMT/CTranslate2/#key-features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\users\\kryst\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\wordcloud-1.8.2.post4+g5dd8d3e-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\users\\kryst\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\wordcloud-1.8.2.post4+g5dd8d3e-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install pytube --quiet\n",
    "%pip install faster-whisper --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\kryst\\\\Documents\\\\PIMPS\\\\tutorials\\\\.\\\\rick.mp3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets download the video\n",
    "from pytube import YouTube\n",
    "\n",
    "YouTube(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\").streams.filter(only_audio=True)[0].download(output_path=\".\", filename=\"rick.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faster_whisper\n",
    "\n",
    "model = faster_whisper.WhisperModel(\"small.en\", compute_type=\"float32\", device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What models are available?\n",
    "\n",
    "Any whisper model compatible with transformers library https://github.com/guillaumekln/faster-whisper#model-conversion\n",
    "\n",
    "Here are models that are already quantized https://huggingface.co/collections/guillaumekln/faster-whisper-64f9c349b3115b4f51434976\n",
    "\n",
    "For Polish you could probably use one of these https://huggingface.co/models?sort=trending&search=whisper+pl\n",
    "\n",
    "But keep in mind that original Whisper is multilingual! It is of course not as good as English but might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\"rick.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscriptionInfo(language='en', language_probability=1, duration=212.1839375, duration_after_vad=212.1839375, all_language_probs=None, transcription_options=TranscriptionOptions(beam_size=5, best_of=5, patience=1, length_penalty=1, repetition_penalty=1, no_repeat_ngram_size=0, log_prob_threshold=-1.0, no_speech_threshold=0.6, compression_ratio_threshold=2.4, condition_on_previous_text=True, prompt_reset_on_temperature=0.5, temperatures=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], initial_prompt=None, prefix=None, suppress_blank=True, suppress_tokens=[-1], without_timestamps=False, max_initial_timestamp=1.0, word_timestamps=False, prepend_punctuations='\"\\'“¿([{-', append_punctuations='\"\\'.。,，!！?？:：”)]}、'), vad_options=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 27.00s]  We're no strangers to love You know the rules, and so do I\n",
      "[27.00s -> 35.00s]  I feel commitments when I'm thinking of You wouldn't get this from any other guy\n",
      "[35.00s -> 43.00s]  I just wanna tell you how I'm feeling Gotta make you understand\n",
      "[43.00s -> 47.00s]  Never gonna give you up Never gonna let you down\n",
      "[47.00s -> 53.00s]  Never gonna run around and desert you Never gonna make you cry\n",
      "[53.00s -> 60.00s]  Never gonna say goodbye Never gonna tell a lie and hurt you\n",
      "[60.00s -> 67.00s]  We've known each other for so long Your heart's been aching us\n",
      "[67.00s -> 73.00s]  You're too shy to say it Inside we both know what's been going on\n",
      "[73.00s -> 82.00s]  We know the game and we're gonna play it And if you ask me how I'm feeling\n",
      "[82.00s -> 87.00s]  Don't tell me you're too blind to see Never gonna give you up\n",
      "[87.00s -> 93.00s]  Never gonna let you down Never gonna run around and desert you\n",
      "[93.00s -> 98.00s]  Never gonna make you cry Never gonna say goodbye\n",
      "[98.00s -> 104.00s]  Never gonna tell a lie and hurt you Never gonna give you up\n",
      "[104.00s -> 110.00s]  Never gonna let you down Never gonna run around and desert you\n",
      "[110.00s -> 114.00s]  Never gonna make you cry Never gonna say goodbye\n",
      "[114.00s -> 119.00s]  Never gonna tell a lie and hurt you\n",
      "[128.00s -> 136.00s]  Never gonna give, never gonna give Never gonna give, never gonna give\n",
      "[137.00s -> 143.00s]  We've known each other for so long Your heart's been aching us\n",
      "[143.00s -> 149.00s]  You're too shy to say it Inside we both know what's been going on\n",
      "[149.00s -> 158.00s]  We know the game and we're gonna play it I just wanna tell you how I'm feeling\n",
      "[158.00s -> 163.00s]  Gotta make you understand Never gonna give you up\n",
      "[163.00s -> 169.00s]  Never gonna let you down Never gonna run around and desert you\n",
      "[169.00s -> 174.00s]  Never gonna make you cry Never gonna say goodbye\n",
      "[174.00s -> 180.00s]  Never gonna tell a lie and hurt you Never gonna give you up\n",
      "[180.00s -> 186.00s]  Never gonna let you down Never gonna run around and desert you\n",
      "[186.00s -> 191.00s]  Never gonna make you cry Never gonna say goodbye\n",
      "[191.00s -> 197.00s]  Never gonna tell a lie and hurt you Never gonna give you up\n",
      "[197.00s -> 203.00s]  Never gonna let you down Never gonna run around and desert you\n",
      "[203.00s -> 208.00s]  Never gonna make you cry Never gonna say goodbye\n",
      "[208.00s -> 211.00s]  Never gonna tell a lie and hurt\n"
     ]
    }
   ],
   "source": [
    "segments = list(segments)\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice but let's convert it into a common standard - srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\users\\kryst\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\wordcloud-1.8.2.post4+g5dd8d3e-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~pencv-python (c:\\Users\\kryst\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install srt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srt\n",
    "\n",
    "subtitles = []\n",
    "\n",
    "for segment in segments:\n",
    "\n",
    "    subtitles.append(srt.Subtitle(\n",
    "        index=len(subtitles) + 1,\n",
    "        start=srt.timedelta(seconds=segment.start),\n",
    "        end=srt.timedelta(seconds=segment.end),\n",
    "        content=segment.text\n",
    "    ))\n",
    "\n",
    "with open(\"rick.srt\", \"w\") as f:\n",
    "    f.write(srt.compose(subtitles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transcribing on edge I would recommend https://github.com/ggerganov/whisper.cpp but I will not go into detail"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local LLMS\n",
    "\n",
    "Note: LLM's are big\n",
    "\n",
    "1B parameters = approx. 1GB RAM\n",
    "\n",
    "Check your memory constraints on the device you are using\n",
    "\n",
    "\n",
    "For model inference on CPU's I will use [llama.cpp](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "### Prerequisities\n",
    "\n",
    "Installation - https://github.com/ggerganov/llama.cpp/releases\n",
    "\n",
    "There are also docker images available\n",
    "\n",
    "\n",
    "#### Models\n",
    "\n",
    "all compatible models are available here : https://huggingface.co/models?search=gguf\n",
    "\n",
    "If you find one that you want to test out just download the file in Files and versions section\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "My personal recommendations:\n",
    "\n",
    "https://huggingface.co/TheBloke/vicuna-7B-v1.5-GGUF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have all the files ready, for simplicity we will start with server implementation\n",
    "\n",
    "Run server.exe from the llama.cpp package you downloaded\n",
    "\n",
    "In the terminal, replace path with appropriate model \n",
    "\n",
    "./server.exe -m vicuna-7b-v1.5.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hey can you help me with something?\n",
      "Result:  I'm trying to write a script that will allow users to upload files and then process them using some custom code. nobody knows what the file is or its contents, just the file name and extension. Is there any way to do this in python without having to manually parse the file for each user?\n",
      "\n",
      "Ideally I would like to be able to run a script on the uploaded files that will extract specific information from them (like the number of lines, words, characters, etc) and then store that information in a database.\n",
      "\n",
      "Is there any library or package that can help me with this task?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "prompt = \"Hey can you help me with something?\"\n",
    "data_json = { \"prompt\": prompt, \"temperature\": 0.1, \"n_predict\": 512, \"stream\": False }\n",
    "\n",
    "resp = requests.post(\n",
    "    url=\"http://127.0.0.1:8080/completion\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json=data_json,\n",
    ")\n",
    "result = resp.json()[\"content\"]\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have set the following parameters:\n",
    "\n",
    "- temperature - it controls randomness in generations\n",
    "- n_predict - number of tokens at which generation will terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to make LLM do some work for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Summarize Commission's stance on AI from the following passages\n",
      "\n",
      "Passage: The Commission is of the opinion that  a given AI application should generally be considered high -risk \n",
      "in light of what is at stake, considering whether both the sector and the intended  use involve \n",
      "significant risks , in particular from the viewpoint of protection of safety , con sumer rights  and \n",
      "fundamental rights .\n",
      "\n",
      "\n",
      "Passage: The Commission is of the opinion that the legislative framework could be improved to address  the \n",
      "following risks and situations : \n",
      " Effective application and enforcement of existing EU and national legislation : the key \n",
      "characteristics of AI create challenges for ensuring the proper application and enforcement of \n",
      "EU and national legislation.\n",
      "\n",
      "\n",
      "Passage: Their development \n",
      "and functioning must be such to ensure that AI systems behave reliably as intended.All reasonable \n",
      "measures should be taken to minimise the risk of harm being caused.\n",
      "\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "def evaluate_prompt(prompt):\n",
    "    data_json = { \"prompt\": prompt, \"temperature\": 0.1, \"n_predict\": 512, \"stream\": False }\n",
    "\n",
    "    resp = requests.post(\n",
    "        url=\"http://127.0.0.1:8080/completion\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=data_json,\n",
    "    )\n",
    "    result = resp.json()[\"content\"]\n",
    "    return result\n",
    "\n",
    "passages = [\n",
    "    \"\"\"The Commission is of the opinion that  a given AI application should generally be considered high -risk \n",
    "in light of what is at stake, considering whether both the sector and the intended  use involve \n",
    "significant risks , in particular from the viewpoint of protection of safety , con sumer rights  and \n",
    "fundamental rights .\"\"\",\n",
    "    \"\"\"The Commission is of the opinion that the legislative framework could be improved to address  the \n",
    "following risks and situations : \n",
    " Effective application and enforcement of existing EU and national legislation : the key \n",
    "characteristics of AI create challenges for ensuring the proper application and enforcement of \n",
    "EU and national legislation.\"\"\",\n",
    "    \"\"\"Their development \n",
    "and functioning must be such to ensure that AI systems behave reliably as intended.All reasonable \n",
    "measures should be taken to minimise the risk of harm being caused.\"\"\"]\n",
    "\n",
    "def create_prompt(task, passages):\n",
    "    prompt = f\"Task: {task}\\n\\n\"\n",
    "    prompt += \"\\n\\n\".join([f\"Passage: {passage}\\n\" for passage in passages])\n",
    "    prompt += \"\\n\\nAnswer: \"\n",
    "    return prompt\n",
    "\n",
    "prompt = create_prompt(\"Summarize Commission's stance on AI from the following passages\", passages)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The Commission's stance on AI is that it considers high-risk applications to be those that involve significant risks, particularly in terms of safety, consumer rights, and fundamental rights. It believes that the legislative framework needs to be improved to address the challenges posed by AI systems, including ensuring effective application and enforcement of existing laws. The Commission also emphasizes the importance of developing and deploying AI systems that are reliable and minimize the risk of harm.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are good at tasks related to language\n",
    "\n",
    "If we use them to answer questions specific to a domain without providing context or use them for reasoning we will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\":\\n nobody knows what it is, and there are no known ways to make it. It is a mystery that has puzzled scientists for decades. The Commission believes that AI is not something that can be created or controlled by humans. It is a force of nature, like electricity or gravity, that exists independently of human consciousness.\\nThe Commission also believes that AI is not something that can be used for good or evil. It is simply a tool that can be used to achieve certain goals, whether those goals are noble or ignoble. The Commission sees AI as a neutral force that can be used for either positive or negative purposes, depending on how it is used.\\nThe Commission's stance on AI is that it is a mystery that cannot be controlled by humans and that it has the potential to be used for both good and evil.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_prompt(\"Summarize Commission's stance on AI from the following passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model I'm using - Vicuna has context length of 4096\n",
    "\n",
    "Let's see what happens if we exceed that number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16278"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = r\"https://raw.githubusercontent.com/ggerganov/llama.cpp/master/examples/server/README.md\"\n",
    "\n",
    "resp = requests.get(url)\n",
    "text = resp.text\n",
    "\n",
    "prompt = f\"Task: What are the main features of the llama server?\\n\\nPassage: {text}\\n\\nAnswer: \"\n",
    "\n",
    "len(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a good explanation on what happens inside along with explanation on attention:\n",
    "\n",
    "https://www.youtube.com/watch?v=f23sUViqxH8\n",
    "\n",
    "As of november 2023 llama.cpp does not support these streaming models, but it supports caching which I will go into detail in next tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis is a Python script that creates an API server using the OpenAPI specification (OAI). The OAI specifies how to build and document RESTful APIs. This script uses the `openapi3` library to generate the API documentation, and the `Flask` library to create the API server.\\n\\nThe script defines a single endpoint `/text_completion` that accepts a POST request with a JSON payload containing a `prompt` field and a `max_tokens` field. The `prompt` field is a string that contains the text for which the API should generate completions, and the `max_tokens` field is an integer that specifies the maximum number of tokens (words or symbols) to return in the completion.\\n\\nThe `/text_completion` endpoint uses the `llama` library to generate completions based on the provided prompt. The `llama` library is a CLI tool that can be used to generate text completions for any given input. It takes a JSON payload with a `prompt` field and returns a JSON response containing an array of completion suggestions.\\n\\nThe script then uses the `Flask` library to create a server that listens on port 5000. The server responds to incoming requests by calling the `/text_completion` endpoint and passing in the provided prompt and max\\\\_tokens values. The response from the `/text_completion` endpoint is then formatted as HTML and returned to the client.\\n\\nTo use this script, you can run it with the following command:\\n```\\npython3 server.py\\n```\\nThis will start the API server on port 5000. You can then send a POST request to `http://localhost:5000/text_completion` with a JSON payload containing a `prompt` field and a `max_tokens` field, and receive a response in HTML format with the generated completions for the provided prompt.\\n\\nNote that you will need to install the `openapi3`, `Flask`, and `llama` libraries before running this script. You can do this by running the following commands:\\n```\\npip install openapi3\\npip install Flask\\npip install llama\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It loses context, also generation time is very long because of recomputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more options for LLM server here:\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md\n",
    "\n",
    "I encourage you to test out your own prompts, models etc. \n",
    "\n",
    "I will go into detail into sampling methods, prompt engineering next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
